{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "---\n",
    "\n",
    "You can find below the code that was used to generate the activity of place cells on a linear track.\n",
    "Use the code and the decoding procedure you lerned about in the lesson to explore how different features of the data impact our ability to decode position.\n",
    "In particular:\n",
    "\n",
    "A - Try to use different fractions of our data samples. How does the median error change when the the number of available sample gets larger? You do not need to re-generate any data, just randomly sub-sample the data to different fractions.\n",
    "\n",
    "B - How many place cells do we need to reliably decode position? Try to re-do the decoding using only 10 cell, then 20, and so on. How does the median error change? Does it reach an asymptote? (Also in this case, you do not need to re-generate the data, you can just select a random subset of cells each time)\n",
    "\n",
    "C - Generate new data using the code below, changing the firing rate noise (changing the value of the variable `noise firing_rate`). How does this noise impact the decoding? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import scipy.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n",
    "from scipy.stats import poisson\n",
    "\n",
    "sys.path.append(os.path.abspath(\"code\"))\n",
    "from utils import download_data\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_length = 200. # the length of our linear track (eg in centimeter)\n",
    "average_firing_rate = 5 # the peak firing rate, averaged across the population \n",
    "n_cells = 100 # how many cells we are recording\n",
    "pf_centers = np.random.rand(n_cells) * track_length # the centers of the place fields for all cells drawn randomly with a uniform distribution on the track\n",
    "pf_size = np.random.gamma(10, size=n_cells) # the size (width) of the place fields, drawn randomly from a gamma distribution \n",
    "pf_rate = np.random.exponential(scale=average_firing_rate, size=n_cells) # the peak firing rate for each cell, drawn from an exponential distribution\n",
    "\n",
    "\n",
    "bins = np.arange(0., 200.)\n",
    "true_firing_rate_maps = np.zeros((n_cells, len(bins)))\n",
    "for i in range(n_cells):\n",
    "    true_firing_rate_maps[i,:] = pf_rate[i] * np.exp(-((bins-pf_centers[i])**2)/(2*pf_size[i]**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE TRAJECTORY\n",
    "\n",
    "n_runs = 10\n",
    "use_stops = False\n",
    "av_running_speed = 10 # the average running speed (in cm/s)\n",
    "fps = 10 # the number of \"video frames\" per second \n",
    "running_speed_a = np.random.chisquare(10, size=n_runs) # running speed in the two directions\n",
    "running_speed_b = np.random.chisquare(10, size=n_runs) \n",
    "\n",
    "stopping_time_a = np.random.chisquare(15, size=n_runs) # the time the mouse will spend at the two ends of the track\n",
    "stopping_time_b = np.random.chisquare(15, size=n_runs)\n",
    "\n",
    "x = np.array([])\n",
    "\n",
    "\n",
    "for i in range(n_runs):\n",
    "    stop1 = np.ones((int(stopping_time_a[i]*fps),)) * 0.\n",
    "    run_length = len(bins) * fps / running_speed_a[i]\n",
    "    run1 = np.linspace(0., float(len(bins)-1), int(run_length))\n",
    "    stop2 = np.ones((int(stopping_time_b[i]*fps),)) * (len(bins)-1.)\n",
    "    run_length = len(bins) * fps / running_speed_b[i]\n",
    "    run2 = np.linspace(len(bins)-1., 0., int(run_length))\n",
    "    if use_stops:\n",
    "        x = np.concatenate((x, stop1, run1, stop2, run2))\n",
    "    else:\n",
    "         x = np.concatenate((x, run1, run2))\n",
    "t = np.arange(len(x))/fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 10000.\n",
    "t_sampling = np.arange(0, t[-1], 1. / sampling_rate)\n",
    "x_sampling = np.floor(np.interp(t_sampling, t, x))\n",
    "noise_firing_rate = 0.1 # the baseline noise firing rate\n",
    "spikes = []\n",
    "\n",
    "for i in range(n_cells):\n",
    "    inst_rate = true_firing_rate_maps[i,x_sampling.astype(np.int32)] + noise_firing_rate\n",
    "    spikes_loc = np.random.poisson(inst_rate/sampling_rate)\n",
    "    sp = np.argwhere(spikes_loc)\n",
    "    t_sp = t_sampling[sp]\n",
    "    spikes.append(t_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_positions = [np.interp(s, t, x) for s in spikes]\n",
    "space_bins = np.arange(0., track_length, 5.) # binning in bins of 5 cms\n",
    "\n",
    "spikes_hist= [np.histogram(s, space_bins)[0] for s in spike_positions]\n",
    "spikes_hist = np.asarray(spikes_hist)\n",
    "\n",
    "occupancy = np.histogram(x, space_bins)[0] /  fps\n",
    "\n",
    "firing_rate_maps = spikes_hist / occupancy \n",
    "\n",
    "spikes_count= [np.histogram(s,t)[0] for s in spikes]\n",
    "spikes_count = np.asarray(spikes_count).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4891/4891 [00:03<00:00, 1295.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6981132075471734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "true_x_original = x[:-1] # get rid of last timepoint to have same length as binned spikes\n",
    "decoding_times = t[:-1]\n",
    "x_decoded_original = np.zeros_like(true_x_original)\n",
    "\n",
    "for t_bin in tqdm(range(len(decoding_times))): # disable False/True: to show/not-show the progress bars\n",
    "    if sum(spikes_count[t_bin,:]) > 0: # Check if the time window contains spikes\n",
    "        posterior = np.empty(firing_rate_maps.shape[-1])\n",
    "        for i in range(len(posterior)):\n",
    "            # Note that we work with log so that we can sum probabilities instead of multiplying them \n",
    "            posterior[i] = np.sum(poisson.logpmf(spikes_count[t_bin,:], firing_rate_maps[:,i]/fps) + 1e-15)\n",
    "        x_decoded_original[t_bin] = space_bins[np.argmax(posterior)]\n",
    "    else:\n",
    "        x_decoded_original[t_bin] = np.nan \n",
    "\n",
    "# compute decoding error only for sampled indices\n",
    "mse_original = np.sqrt((true_x_original - x_decoded_original)**2)\n",
    "median_error_original = np.nanmedian(mse_original)\n",
    "\n",
    "print(median_error_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until here, nothing is new. We now do A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "true_positions = x[:-1]\n",
    "time_bins = t[:-1]\n",
    "\n",
    "# Sampling fractions to test\n",
    "sample_size = [0.01, 0.1, 0.15, 0.3, 0.5, 0.75, 0.8, 0.9]\n",
    "med_errors_final = []\n",
    "\n",
    "num_realizations = 5  # Number of times to repeat the experiment\n",
    "\n",
    "# Loop over each sampling fraction\n",
    "for i in sample_size:\n",
    "    realization_errors = []\n",
    "    \n",
    "    for realization_idx in range(num_realizations):\n",
    "        # Randomly select time bin indices based on the current fraction\n",
    "        num_sampled_bins = int(len(time_bins) * i)\n",
    "        sampled_time_indices = np.sort(np.random.choice(len(time_bins), num_sampled_bins, replace=False))\n",
    "        \n",
    "        decoded_positions = np.zeros(np.shape(true_positions))\n",
    "        \n",
    "        # Decode position for each sampled time bin\n",
    "        for time_idx in tqdm(sampled_time_indices, disable=True):\n",
    "            if np.sum(spikes_count[time_idx, :]) > 0:  # Only decode if there are spikes\n",
    "                posterior_log_probs = np.empty(firing_rate_maps.shape[-1])\n",
    "                \n",
    "                # Compute log-posterior for each spatial bin\n",
    "                for bin_idx in range(len(posterior_log_probs)):\n",
    "                    firing_rate = firing_rate_maps[:, bin_idx] / fps\n",
    "                    log_likelihood = poisson.logpmf(spikes_count[time_idx, :], firing_rate) + 1e-15\n",
    "                    posterior_log_probs[bin_idx] = np.sum(log_likelihood)\n",
    "                \n",
    "                # Decode position as the bin with maximum posterior probability\n",
    "                decoded_positions[time_idx] = space_bins[np.argmax(posterior_log_probs)]\n",
    "            else:\n",
    "                decoded_positions[time_idx] = np.nan\n",
    "        \n",
    "        # Compute decoding error for sampled indices\n",
    "        decoding_errors = np.sqrt((true_positions[sampled_time_indices] - decoded_positions[sampled_time_indices]) ** 2)\n",
    "        median_error = np.nanmedian(decoding_errors)\n",
    "        \n",
    "        realization_errors.append(median_error)\n",
    " \n",
    "    mean_median_error = np.mean(realization_errors)\n",
    "    med_errors_final.append(mean_median_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled fraction: 1%, Median error: 3.78 cm\n",
      "Sampled fraction: 10%, Median error: 3.62 cm\n",
      "Sampled fraction: 15%, Median error: 3.81 cm\n",
      "Sampled fraction: 30%, Median error: 3.68 cm\n",
      "Sampled fraction: 50%, Median error: 3.73 cm\n",
      "Sampled fraction: 75%, Median error: 3.71 cm\n",
      "Sampled fraction: 80%, Median error: 3.70 cm\n",
      "Sampled fraction: 90%, Median error: 3.69 cm\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sample_size)):\n",
    "    frac = sample_size[i]\n",
    "    median_error = med_errors_final[i]\n",
    "    print(f\"Sampled fraction: {frac*100:.0f}%\"+f\", Median error: {median_error:.2f} cm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to B)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_c(n):\n",
    "    track_length = 200. # the length of our linear track (eg in centimeter)\n",
    "    average_firing_rate = 5 # the peak firing rate, averaged across the population \n",
    "    n_cells = n # how many cells we are recording\n",
    "    pf_centers = np.random.rand(n_cells) * track_length # the centers of the place fields for all cells drawn randomly with a uniform distribution on the track\n",
    "    pf_size = np.random.gamma(10, size=n_cells) # the size (width) of the place fields, drawn randomly from a gamma distribution \n",
    "    pf_rate = np.random.exponential(scale=average_firing_rate, size=n_cells) # the peak firing rate for each cell, drawn from an exponential distribution\n",
    "\n",
    "\n",
    "    bins = np.arange(0., 200.)\n",
    "    true_firing_rate_maps = np.zeros((n_cells, len(bins)))\n",
    "    for i in range(n_cells):\n",
    "        true_firing_rate_maps[i,:] = pf_rate[i] * np.exp(-((bins-pf_centers[i])**2)/(2*pf_size[i]**2))\n",
    "\n",
    "    n_runs = 10\n",
    "    use_stops = False\n",
    "    av_running_speed = 10 # the average running speed (in cm/s)\n",
    "    fps = 10 # the number of \"video frames\" per second \n",
    "    running_speed_a = np.random.chisquare(10, size=n_runs) # running speed in the two directions\n",
    "    running_speed_b = np.random.chisquare(10, size=n_runs) \n",
    "\n",
    "    stopping_time_a = np.random.chisquare(15, size=n_runs) # the time the mouse will spend at the two ends of the track\n",
    "    stopping_time_b = np.random.chisquare(15, size=n_runs)\n",
    "\n",
    "    x = np.array([])\n",
    "\n",
    "\n",
    "    for i in range(n_runs):\n",
    "        stop1 = np.ones((int(stopping_time_a[i]*fps),)) * 0.\n",
    "        run_length = len(bins) * fps / running_speed_a[i]\n",
    "        run1 = np.linspace(0., float(len(bins)-1), int(run_length))\n",
    "        stop2 = np.ones((int(stopping_time_b[i]*fps),)) * (len(bins)-1.)\n",
    "        run_length = len(bins) * fps / running_speed_b[i]\n",
    "        run2 = np.linspace(len(bins)-1., 0., int(run_length))\n",
    "        if use_stops:\n",
    "            x = np.concatenate((x, stop1, run1, stop2, run2))\n",
    "        else:\n",
    "            x = np.concatenate((x, run1, run2))\n",
    "    t = np.arange(len(x))/fps\n",
    "\n",
    "    sampling_rate = 10000.\n",
    "    t_sampling = np.arange(0, t[-1], 1. / sampling_rate)\n",
    "    x_sampling = np.floor(np.interp(t_sampling, t, x))\n",
    "    noise_firing_rate = 0.1 # the baseline noise firing rate\n",
    "    spikes = []\n",
    "\n",
    "    for i in range(n_cells):\n",
    "        inst_rate = true_firing_rate_maps[i,x_sampling.astype(np.int32)] + noise_firing_rate\n",
    "        spikes_loc = np.random.poisson(inst_rate/sampling_rate)\n",
    "        sp = np.argwhere(spikes_loc)\n",
    "        t_sp = t_sampling[sp]\n",
    "        spikes.append(t_sp)\n",
    "\n",
    "    spike_positions = [np.interp(s, t, x) for s in spikes]\n",
    "    space_bins = np.arange(0., track_length, 5.) # binning in bins of 5 cms\n",
    "\n",
    "    spikes_hist= [np.histogram(s, space_bins)[0] for s in spike_positions]\n",
    "    spikes_hist = np.asarray(spikes_hist)\n",
    "\n",
    "    occupancy = np.histogram(x, space_bins)[0] /  fps\n",
    "\n",
    "    firing_rate_maps = spikes_hist / occupancy \n",
    "\n",
    "    spikes_count= [np.histogram(s,t)[0] for s in spikes]\n",
    "    spikes_count = np.asarray(spikes_count).T \n",
    "\n",
    "    true_x_original = x[:-1] # get rid of last timepoint to have same length as binned spikes\n",
    "    decoding_times = t[:-1]\n",
    "    x_decoded_original = np.zeros_like(true_x_original)\n",
    "\n",
    "    for t_bin in tqdm(range(len(decoding_times))): # disable False/True: to show/not-show the progress bars\n",
    "        if sum(spikes_count[t_bin,:]) > 0: # Check if the time window contains spikes\n",
    "            posterior = np.empty(firing_rate_maps.shape[-1])\n",
    "            for i in range(len(posterior)):\n",
    "                # Note that we work with log so that we can sum probabilities instead of multiplying them \n",
    "                posterior[i] = np.sum(poisson.logpmf(spikes_count[t_bin,:], firing_rate_maps[:,i]/fps) + 1e-15)\n",
    "            x_decoded_original[t_bin] = space_bins[np.argmax(posterior)]\n",
    "        else:\n",
    "            x_decoded_original[t_bin] = np.nan \n",
    "\n",
    "    # compute decoding error only for sampled indices\n",
    "    mse_original = np.sqrt((true_x_original - x_decoded_original)**2)\n",
    "    median_error_original = np.nanmedian(mse_original)\n",
    "\n",
    "    return median_error_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5806/5806 [00:01<00:00, 3648.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells = 10, median error 7.629742033383906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4462/4462 [00:01<00:00, 2300.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells = 20, median error 7.346289752650179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6111/6111 [00:03<00:00, 1835.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells = 30, median error 7.146666666666675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4512/4512 [00:03<00:00, 1439.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells = 40, median error 5.005586592178773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4888/4888 [00:03<00:00, 1380.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells = 50, median error 4.533846574344025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4362/4362 [00:03<00:00, 1374.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells = 60, median error 4.651296967385089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5018/5018 [00:03<00:00, 1312.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells = 70, median error 4.0847457627118615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4383/4383 [00:03<00:00, 1311.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells = 80, median error 3.770833333333343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4068/4068 [00:03<00:00, 1292.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells = 90, median error 3.532523102954574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [10,20,30,40,50,60,70,80,90]:\n",
    "    error = n_c(i)\n",
    "\n",
    "    print(\"Number of cells = \"+str(i)+\", median error \"+str(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median approaches the value 3.5, like we saw it in the course notebook for 100% of the cells. For less place cell we observe quite high values, above 7.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noises(noise):\n",
    "    spikes = []\n",
    "    n_cells = 100\n",
    "    for i in range(n_cells):\n",
    "        inst_rate = true_firing_rate_maps[i,x_sampling.astype(np.int32)] + noise\n",
    "        spikes_loc = np.random.poisson(inst_rate/sampling_rate)\n",
    "        sp = np.argwhere(spikes_loc)\n",
    "        t_sp = t_sampling[sp]\n",
    "        spikes.append(t_sp)\n",
    "\n",
    "    spike_positions = [np.interp(s, t, x) for s in spikes]\n",
    "    space_bins = np.arange(0., track_length, 5.) \n",
    "    spikes_hist= [np.histogram(s, space_bins)[0] for s in spike_positions]\n",
    "    spikes_hist = np.asarray(spikes_hist)\n",
    "    occupancy = np.histogram(x, space_bins)[0] /  fps\n",
    "    firing_rate_maps = spikes_hist / occupancy \n",
    "\n",
    "    spikes_count= [np.histogram(s,t)[0] for s in spikes]\n",
    "    spikes_count = np.asarray(spikes_count).T \n",
    "\n",
    "    true_x = x[:-1] \n",
    "    decoding_times = t[:-1]\n",
    "    x_decoded = np.zeros_like(true_x)\n",
    "\n",
    "    for t_bin in tqdm(range(len(decoding_times))):\n",
    "        if sum(spikes_count[t_bin,:])>0:\n",
    "            posterior = np.empty(firing_rate_maps.shape[-1])\n",
    "            for i in range(len(posterior)):\n",
    "                posterior[i] = sum(poisson.logpmf(spikes_count[t_bin,:],firing_rate_maps[:,i]/fps)+pow(1,-15))\n",
    "            x_decoded[t_bin] = space_bins[np.argmax(posterior)]\n",
    "        else:\n",
    "            x_decoded[t_bin] = np.nan   \n",
    "\n",
    "\n",
    "    mse = np.sqrt((true_x-x_decoded)**2)\n",
    "    median_error = np.nanmedian(mse)\n",
    "\n",
    "    return median_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4891/4891 [00:04<00:00, 1062.24it/s]\n",
      "100%|██████████| 4891/4891 [00:04<00:00, 1095.35it/s]\n",
      "100%|██████████| 4891/4891 [00:04<00:00, 1086.01it/s]\n",
      "100%|██████████| 4891/4891 [00:04<00:00, 1097.24it/s]\n",
      "100%|██████████| 4891/4891 [00:04<00:00, 1069.69it/s]\n",
      "100%|██████████| 4891/4891 [00:04<00:00, 1091.57it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = np.linspace(0.05, 10.0, 6)\n",
    "\n",
    "for i in n:\n",
    "    median_error = noises(i)\n",
    "    plt.plot(i,median_error, '.')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a what seems to be linear increase of the median error with the noise, as we would have expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_name = 'linear_track_data.pickle' # change this name when you save new data\n",
    "\n",
    "out_data = {}\n",
    "out_data['x'] = x\n",
    "out_data['t'] = t\n",
    "out_data['spikes'] = spikes\n",
    "out_data['track_length'] = track_length\n",
    "out_data['fps'] = fps\n",
    "\n",
    "with open('data/'+file_name,'wb') as f:\n",
    "    pickle.dump(out_data,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "---\n",
    "In the loop implementation of the decoder, we used `poisson.logpmf(k,mu)` to calculate the log probability of observing $k$ spikes given an average firing rate of $\\mu$. \n",
    "This is mathematically equivalent to `np.log(poisson.pmf(k,mu))`, in which we calculate the probability, and then take the log.\n",
    "\n",
    "\n",
    "Re-run the decoding substituting this expression:\n",
    "\n",
    "```\n",
    "posterior[i] = sum(np.log(poisson.pmf(spikes_count[t_bin,:],firing_rate_maps[:,i]/fps)+pow(1,-15)))\n",
    "```\n",
    "\n",
    "To the line we are using to calculate the posterior.\n",
    "Do you see any difference in the results? What do you think this is due to?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "I inserted this into the class notebook, because I didn't want to copy everything over here. If we change the last term in the new line pow(1,-15) to pow(10,-15), which is I think wanted, we don\"t observe a difference. As stated in the exercise, the two expressions are the same, so that could be the reason.\n",
    "\n",
    "If the line is inserted as shown here, we observe very different behaviour and the decoder performs very badly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "---\n",
    "A - Estimate the quality of the sequence detection methods we saw in the lesson. How many false poistive does it find? How many false negatives?\n",
    "\n",
    "B - Investigate the effect of `noise_x_react` and `noise_t_react` on the false positive rate and the false negative rate of our detection procedure.\n",
    "\n",
    "C - What kind of sequence can our methods detect? What kind of activity, despide being sequential, could escape our detection method? Would you have an idea for a different method for sequence detection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: The methods we law in the lesson notebook were on the one hand to fit the activations of the cells in order to find the slope and derive from that if the activaiton was random or if there was a sequence playing and on the other hand the visual evaluation and with that comparing the true sequences with the decoded ones and estimating the median square error. In order to talk about false positives and false negatives, it makes more sense to think of the first method, where we simulated random sequences and some, where the animal actually moved. Here, the start and end point of the movement was randomly determined, thus there were some trajectories were the animal started in x=40cm and ended in x=44cm, this would give a slope of almost 0, eventhough there was very non-random movement. Interestingly, the distribution of the random slopes shows only positive values and also the distribution for the non-random sequences is shifted towards positive values. Since we look at random processes, I would say we have the tolerance to attribute a slope between -1<0<1 to a rdnom process. This would mean we have no false positives (by my definition). In the group with real requences there were around 8% of trails with a slope btween -1 and 1, which I would call a false negatives, because they seem random, but aren't.\n",
    "\n",
    "B: Its shown below how I found the results discussed here. For both varying the noise in the timing of the spikes (t_noise) and the noise of the reactivation of the sequence (x_noise) I didn't observe a growing rate for false negatives, it stay around 8%. This is probably wrong ang goes against my intuiton, but I don't know where the error is.\n",
    "\n",
    "C: Is already kind of answered. If the movement is only over a small part of the map, the slope becomes quite small and eventually falls into my definition of random. If the animal would also be able to run back and forth, the slopes could even out, leaving a slope close to zero. To compensate for that, we could cut the sequence into smaller pieces and determine the slopes for them. If they are all close to zero, the trail is random, if they show a distribution of values, it could be a movement pover the track. This would not fix the case where the animal doesn't move far over the map. There could be an extra check for this special case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3\n",
    "\n",
    "sns.set_theme(context='notebook',style='white',font_scale=1.5,\n",
    "              rc = {'axes.spines.top':False,'axes.spines.right':False})\n",
    "\n",
    "#download lesson data\n",
    "download_data('https://surfdrive.surf.nl/files/index.php/s/TliAW2xObyy0keu')\n",
    "\n",
    "#code: data import\n",
    "data_file = 'data/linear_track_data.pickle'\n",
    "with open(data_file, 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "x,t,spikes = data['x'],data['t'],data['spikes']\n",
    "track_length = data['track_length']\n",
    "fps = data['fps'] \n",
    "n_cells = len(spikes)\n",
    "\n",
    "# we compute the poistion at which each spike was emitted\n",
    "spike_positions = [np.interp(s, t, x) for s in spikes]\n",
    "space_bins = np.arange(0., track_length, 5.) # binning in bins of 5 cms\n",
    "\n",
    "# we compute histograms for each cell\n",
    "spikes_hist= [np.histogram(s, space_bins)[0] for s in spike_positions]\n",
    "spikes_hist = np.asarray(spikes_hist)\n",
    "\n",
    "# we also need an \"occupancy histogram\" in order to normalize the firing rates maps \n",
    "occupancy = np.histogram(x, space_bins)[0] /  fps\n",
    "\n",
    "firing_rate_maps = spikes_hist / occupancy \n",
    "# generate data \n",
    "\n",
    "spikes_count= [np.histogram(s,t)[0] for s in spikes]\n",
    "spikes_count = np.asarray(spikes_count).T # we transpose the matrix to have the more familiar (samples x features) shape\n",
    "\n",
    "def noise_check(x,t):\n",
    "    n_events = 200 # number of reactivation events\n",
    "    event_duration = 100 # in bins\n",
    "    sampling_rate = 100 # sampling rate \n",
    "    t_react = np.arange(0,100)\n",
    "    noise_x_react=x; # Noise in the reactivation of the sequence\n",
    "    noise_t_react=t; # Noise in the timing of the spikes \n",
    "    noise_firing_rate = 0.1 # the baseline noise firing rate\n",
    "\n",
    "\n",
    "    reactivation_events = np.zeros((n_events,event_duration))\n",
    "    spikes_react = np.zeros((n_events,n_cells,event_duration))\n",
    "\n",
    "    for event in range(n_events):\n",
    "        if(event<=n_events//2):\n",
    "        #Generate \"real\" sequences for the first half of events\n",
    "            x_start = np.random.uniform(0,track_length) # Starting point\n",
    "            x_end = np.random.uniform(0,track_length) # Ending point\n",
    "            x_react = np.linspace(x_start,x_end,event_duration) \\\n",
    "                +np.random.normal(0,noise_x_react,size=event_duration)\n",
    "\n",
    "        else:\n",
    "            #Pick locations randomly for the second half \n",
    "            x_react = np.random.uniform(0,track_length,size=event_duration);\n",
    "\n",
    "        x_react[x_react<0]=0;\n",
    "        x_react[x_react>track_length]=track_length\n",
    "        \n",
    "        #store reactivation sequence\n",
    "        reactivation_events[event,:] = x_react\n",
    "\n",
    "        \n",
    "        # Generate spikes according to the location being reactivated in this event\n",
    "        \n",
    "        for i in range(n_cells):\n",
    "            binned_x = np.digitize(x_react,bins=np.linspace(0,track_length,firing_rate_maps.shape[-1]))-1\n",
    "            inst_rate = firing_rate_maps[i,binned_x] + np.random.normal(0,noise_firing_rate,size=len(binned_x))\n",
    "            inst_rate[inst_rate<0] = 0\n",
    "            spikes_loc = np.where(np.random.poisson(inst_rate/sampling_rate)>0)\n",
    "            spikes_loc = spikes_loc + np.round(np.random.normal(0,noise_t_react,size=len(spikes_loc)))\n",
    "            spikes_loc = spikes_loc[np.logical_and(spikes_loc>0,spikes_loc<event_duration)]\n",
    "            spikes_react[event,i,spikes_loc.astype(int)] = 1\n",
    "\n",
    "    time_window = 5 # number of bins to aggregate during decoding\n",
    "\n",
    "    reactivation_slope = np.zeros(n_events)\n",
    "\n",
    "    for event in range(n_events):\n",
    "\n",
    "        event_spikes = spikes_react[event]\n",
    "        # First we bin the events in windows of 10 bins\n",
    "        spikes_sampled = np.zeros((n_cells,event_spikes.shape[1]//time_window))\n",
    "        # We generate a new spike matrix with the re-sized window\n",
    "        for t_r in range(1,event_spikes.shape[1]//time_window):\n",
    "            spikes_sampled[:,t_r] = np.sum(event_spikes[:,(t_r-1)*time_window :(t_r)*time_window],axis=1)\n",
    "\n",
    "    # to do: bayesian decoding\n",
    "    t_resize = 10 # We use spikes from multiple time windows for the decoding\n",
    "\n",
    "    reactivation_slopes = np.zeros(n_events)\n",
    "    reactivation_pvalues = np.zeros(n_events)\n",
    "\n",
    "    for event in range(n_events):\n",
    "\n",
    "        event_spikes = spikes_react[event]\n",
    "        spikes_sampled = np.zeros((n_cells,event_spikes.shape[1]//t_resize))\n",
    "\n",
    "        # We generate a new spike matrix with the re-sized window\n",
    "        for t_r in range(1,event_spikes.shape[1]//t_resize):\n",
    "            spikes_sampled[:,t_r] = np.sum(event_spikes[:,(t_r-1)*t_resize :(t_r)*t_resize],axis=1)\n",
    "\n",
    "\n",
    "        # We then perform decoding on the aggregated spikes\n",
    "        x_decoded = np.zeros(spikes_sampled.shape[1])\n",
    "\n",
    "        for t_bin in range(spikes_sampled.shape[1]):\n",
    "\n",
    "            if sum(spikes_count[t_bin,:])>0: # Check if the time window contains spikes\n",
    "\n",
    "                posterior = np.empty(firing_rate_maps.shape[-1])\n",
    "                for i in range(len(posterior)):\n",
    "                    posterior[i] = sum(poisson.logpmf(spikes_sampled[:,t_bin],firing_rate_maps[:,i]*t_resize/fps)+pow(1,-15))\n",
    "\n",
    "                x_decoded[t_bin] = space_bins[np.argmax(posterior)]\n",
    "\n",
    "            else:\n",
    "                x_decoded[t_bin] = np.nan   \n",
    "\n",
    "            # We fit a line to the decoded positions, and save the slope\n",
    "            slope,_ = np.polyfit(np.arange(len(x_decoded)),x_decoded,deg=1)\n",
    "            reactivation_slopes[event] = slope\n",
    "            # And calculate the pvalue of the pearson correlation\n",
    "            corr = pearsonr(np.arange(len(x_decoded)),x_decoded)\n",
    "            reactivation_pvalues[event] = corr[1]\n",
    "\n",
    "\n",
    "    false_positives = 0\n",
    "    for i in range(len(reactivation_slopes)):\n",
    "        if -1 < reactivation_slopes[i] < 1:\n",
    "            false_positives = false_positives + 1\n",
    "\n",
    "    return (false_positives/len(reactivation_slope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gw/1gkg7kb92xlb5cvtzpqrsmk40000gn/T/ipykernel_3849/1698811854.py:128: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr = pearsonr(np.arange(len(x_decoded)),x_decoded)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise x = 0, mean false positive rate = 0.089\n",
      "noise x = 1, mean false positive rate = 0.07750000000000001\n",
      "noise x = 5, mean false positive rate = 0.08249999999999999\n",
      "noise x = 10, mean false positive rate = 0.07849999999999999\n",
      "noise x = 15, mean false positive rate = 0.08399999999999999\n",
      "noise x = 20, mean false positive rate = 0.089\n"
     ]
    }
   ],
   "source": [
    "#here the t noise is constant at 5, noise x is being varied\n",
    "how_many = 10\n",
    "for i in [0,1,5,10,15,20]:\n",
    "    mean = 0\n",
    "    for _ in range(how_many):\n",
    "        mean = mean + noise_check(i,5)\n",
    "    \n",
    "    print(\"noise x = \"+str(i)+ \", mean false positive rate = \"+str(mean/how_many))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gw/1gkg7kb92xlb5cvtzpqrsmk40000gn/T/ipykernel_3849/1698811854.py:128: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr = pearsonr(np.arange(len(x_decoded)),x_decoded)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise x = 0, mean false positive rate = 0.08399999999999999\n",
      "noise x = 1, mean false positive rate = 0.09999999999999999\n",
      "noise x = 5, mean false positive rate = 0.07799999999999999\n",
      "noise x = 10, mean false positive rate = 0.07550000000000003\n",
      "noise x = 15, mean false positive rate = 0.0845\n",
      "noise x = 20, mean false positive rate = 0.08399999999999999\n"
     ]
    }
   ],
   "source": [
    "#here the x noise is constant at 5, noise t is being varied\n",
    "for i in [0,1,5,10,15,20]:\n",
    "    mean = 0\n",
    "    for _ in range(how_many):\n",
    "        mean = mean + noise_check(5,i)\n",
    "    \n",
    "    print(\"noise x = \"+str(i)+ \", mean false positive rate = \"+str(mean/how_many))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uni)",
   "language": "python",
   "name": "uni"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec757111aa82fc412dab5a41ba1a33fdb6db5c8112df3ff06fec0dbff050b412"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
